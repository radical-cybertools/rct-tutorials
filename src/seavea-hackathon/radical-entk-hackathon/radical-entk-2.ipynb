{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0718844a",
   "metadata": {},
   "source": [
    "## RADICAL-EnTK - Part 2\n",
    "\n",
    "This second part of the tutorial will cover some advanced elements of EnTK.  The reader will be enabled to program  dynamic ensemble applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f65de",
   "metadata": {},
   "source": [
    "EnTK and its runtime system RADICAL-Pilot require MongoDB instance running. It serves need to be deployed and made available before using EnTK. Here we set the access parameters for the service.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note:</b>The following assumes that: 1. you have a shell; 2. you export the relevant environment variables; 3. you execute the command jupyter notebook from that shell. In that way, the relevant env variables will be read here via os.environ.get('NAME_VARIABLE').</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53195bcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T12:17:52.430749Z",
     "start_time": "2022-11-22T12:17:52.422540Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture capt\n",
    "\n",
    "import os\n",
    "\n",
    "mdb_host = os.environ.get('MDB_HOST', 'mongodb')\n",
    "mdb_port = os.environ.get('MDB_PORT', '27017')\n",
    "mdb_name = os.environ.get('MDB_USER', 'guest')\n",
    "mdb_pswd = os.environ.get('MDB_PSWD', 'guest')\n",
    "mdb_dtbs = os.environ.get('MDB_DTBS', 'default')\n",
    "\n",
    "%env RADICAL_PILOT_DBURL=mongodb://$mdb_name:$mdb_pswd@$mdb_host:$mdb_port/$mdb_dtbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf6a6d1",
   "metadata": {},
   "source": [
    "# Example: Ensemble of Simulation Pipelines\n",
    "\n",
    "Similar to the first tutorial exercise, we will implement an ensemble of simulation pipelines where each pipeline prepares data, runs a set of simulation on the data, and then accumulates results.  Other than before though we will add dynamicity to the application:\n",
    "\n",
    "  - after each simulation stage, evaluate intermediate data\n",
    "  - if data diverge (sum is larger than `LIMIT`)\n",
    "    - re-seed the pipeline with new data\n",
    "  - else\n",
    "    - run another simulation steps\n",
    "  - finish simulation after at most `MAX_ITER` steps and then collect results\n",
    "\n",
    "This exercise will thus show how the workflow can be adapted at runtime, by inserting new pipeline stages or stopping the execution of individual pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d40bda",
   "metadata": {},
   "source": [
    "First we import EnTK Python module in our application so to be able to use its API.  We also define a number of global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77692350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T12:17:57.635805Z",
     "start_time": "2022-11-22T12:17:57.340827Z"
    }
   },
   "outputs": [],
   "source": [
    "import radical.entk as re\n",
    "import functools\n",
    "\n",
    "N_PIPELINES   =    5                 # number of simulation pipeline\n",
    "N_SIMULATIONS =    5                 # number of simulations per pipeline\n",
    "MAX_ITER      =   10                 # max number of simulation steps\n",
    "LIMIT         =  100 * 1000 * 1000   # max intermediate result\n",
    "\n",
    "\n",
    "# we want to change pipelines on the fly, thus want to keep track\n",
    "# of all pipelines.  We identify pipelines by an application specified\n",
    "# name `pname`\n",
    "ensemble = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31dfd5c",
   "metadata": {},
   "source": [
    "The following function generates a single simulation pipeline, i.e., a new ensemble member. The pipeline structure consisting of three steps as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a57f3-2f78-443c-b945-def54ac6e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stage_1(sandbox, pname):\n",
    "    '''\n",
    "    Create a stage which seeds (or re-seeds) a simulation pipeline with a random\n",
    "    integer as input data.\n",
    "\n",
    "    The returned stage will include a `post_exec` callback which outputs the new\n",
    "    data seed after completion of the stage\n",
    "    '''\n",
    "\n",
    "    t1 = re.Task()\n",
    "    t1.executable = '/bin/sh'\n",
    "    t1.arguments  = ['-c', 'od -An -N1 -i /dev/random']\n",
    "    t1.stdout     = 'random.txt'\n",
    "    t1.sandbox    = sandbox\n",
    "    t1.download_output_data = ['random.txt > %s.random.txt' % pname]\n",
    "\n",
    "    s1 = re.Stage()\n",
    "    s1.add_tasks(t1)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # use a callback after that stage completed for output of the seed value\n",
    "    def post_exec(stage, pname):\n",
    "        seed = int(open('%s.random.txt' % pname).read().split()[-1])\n",
    "        print(pname, 'rand  --- - %10d' % seed)\n",
    "    # --------------------------------------------------------------------------\n",
    "    s1.post_exec = functools.partial(post_exec, s1, pname)\n",
    "\n",
    "    return s1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de20a02b-4ed9-4023-a6aa-b4d8b3885ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stage_2(sandbox, pname, iteration=0):\n",
    "    '''\n",
    "    The second pipeline stage is again the simulation stage: it consists of\n",
    "    `N_SIMULATIONS` tasks which compute the n'th power of the input data (last\n",
    "    line of `random.txt`).  Another `post_exec` callback will, after all tasks\n",
    "    are done, evaluate the intermediate data and decide how to continue:\n",
    "\n",
    "        - if result    > LIMIT   : new seed  (add stages 1 and 2)\n",
    "        - if iteration > MAX_ITER: abort     (add stage 3)\n",
    "        - else                   : continue  (add stage 2 again)\n",
    "\n",
    "    For simplicity we keep track of iterations within the pipeline instances\n",
    "    (`pipeline.iteration`).  A 'proper' implementation may want to create\n",
    "    a subclass from `re.Pipeline` which hosts that counter as `self._iteration`.\n",
    "    '''\n",
    "\n",
    "    pipeline = ensemble[pname]\n",
    "\n",
    "    # this is the pseudo simulation we iterate on the second stage\n",
    "    sim = 'echo \"($(tail -qn 1 random.txt) + %(iteration)d) ^ %(ensemble_id)d\"'\\\n",
    "          '| bc'\n",
    "\n",
    "    # second stage: create N_SIMULATIONS tasks to compute the n'th power\n",
    "    s2 = re.Stage()\n",
    "    pipeline.iteration = iteration                                # type: ignore\n",
    "    for i in range(N_SIMULATIONS):\n",
    "        t2 = re.Task()\n",
    "        t2.executable = '/bin/sh'\n",
    "        t2.arguments  = ['-c', sim % {'iteration'  : iteration,\n",
    "                                      'ensemble_id': i}]\n",
    "        t2.stdout     = 'power.%03d.txt' % i\n",
    "        t2.sandbox    = sandbox\n",
    "        t2.download_output_data = ['%s > %s.%s' % (t2.stdout, pname, t2.stdout)]\n",
    "        s2.add_tasks(t2)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # add a callback after that stage's completed which checks the\n",
    "    # intermediate results:\n",
    "    def post_exec(stage, pname):\n",
    "\n",
    "        pipeline = ensemble[pname]\n",
    "        iteration = pipeline.iteration\n",
    "\n",
    "        # continue to iterate - check intermediate data\n",
    "        result = 0\n",
    "        for task in stage.tasks:\n",
    "            data    = open('%s.%s' % (pname, task.stdout)).read()\n",
    "            result += int(data.split()[-1])\n",
    "\n",
    "        if result > LIMIT:\n",
    "            # simulation diverged = reseed the pipeline (add new stages 1 and 2)\n",
    "            print(pipeline.name, 'seed  %3d - %10d' % (iteration, result))\n",
    "            pipeline.add_stages(get_stage_1(sandbox, pname))\n",
    "            pipeline.add_stages(get_stage_2(sandbox, pname))\n",
    "\n",
    "        elif iteration > MAX_ITER:\n",
    "            # iteration limit reached, discontinue pipeline (add final stage 3)\n",
    "            print(pipeline.name, 'break %3d - %10d' % (iteration, result))\n",
    "            pipeline.add_stages(get_stage_3(sandbox, pname))\n",
    "\n",
    "        else:\n",
    "            # continue to iterate (increase the iteration counter)\n",
    "            print(pipeline.name, 'iter  %3d - %10d' % (iteration, result))\n",
    "            pipeline.add_stages(get_stage_2(sandbox, pname, iteration + 1))\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    s2.post_exec = functools.partial(post_exec, s2, pname)\n",
    "\n",
    "    return s2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a19205-7a0c-4e29-8ffc-73194e3ab69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stage_3(sandbox, pname):\n",
    "    '''\n",
    "    This pipeline has reached its iteration limit without exceeding the\n",
    "    simulation results diverging beyond `LIMIT`.  This final stage will collect\n",
    "    the simulation results and report the final data, again via an `post-exec`\n",
    "    hook.\n",
    "    '''\n",
    "\n",
    "    # third stage: compute sum over all powers\n",
    "    t3 = re.Task()\n",
    "    t3.executable = '/bin/sh'\n",
    "    t3.arguments  = ['-c', 'tail -qn 1 power.*.txt | paste -sd+ | bc']\n",
    "    t3.stdout     = 'sum.txt'\n",
    "    t3.sandbox    = sandbox\n",
    "    t3.download_output_data = ['%s > %s.%s' % (t3.stdout, pname, t3.stdout)]\n",
    "\n",
    "    # download the result while renaming to get unique files per pipeline\n",
    "    t3.download_output_data = ['sum.txt > %s.sum.txt' % pname]\n",
    "\n",
    "    s3 = re.Stage()\n",
    "    s3.add_tasks(t3)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # use a callback after that stage completed for output of the final result\n",
    "    def post_exec(stage, pname):\n",
    "        result = int(open('%s.sum.txt' % pname).read())\n",
    "        print(pname, 'final %3d - %10d' % (MAX_ITER, result))\n",
    "    # --------------------------------------------------------------------------\n",
    "    s3.post_exec = functools.partial(post_exec, s3, pname)\n",
    "\n",
    "    return s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396cb007",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T12:17:59.795986Z",
     "start_time": "2022-11-22T12:17:59.787028Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_pipeline(pname):\n",
    "    '''\n",
    "    We generate essentially the same pipeline as in `radical_entk_1.py` with\n",
    "    three stages:\n",
    "\n",
    "      1) generate a random seed as input data\n",
    "      2) evolve a model based on that input data via a set of ensembles\n",
    "      3) derive a common metric across the model results\n",
    "\n",
    "    However, we will iterate the model (stage 2) multiple times and check for\n",
    "    intermediate results.  Further, we will cancel all pipelines whose\n",
    "    intermediate result is larger than some threshold and will instead replace\n",
    "    that pipeline with a newly seeded pipeline.  We break after a certain number\n",
    "    of iterations and expect the result to be biased toward smaller seeds.\n",
    "    '''\n",
    "\n",
    "    # create and register pipeline\n",
    "    p = re.Pipeline()\n",
    "    p.name = pname\n",
    "    ensemble[pname] = p\n",
    "\n",
    "    # all tasks in this pipeline share the same sandbox\n",
    "    sandbox = pname\n",
    "\n",
    "    # first stage: create 1 task to generate a random seed number\n",
    "    s1 = get_stage_1(sandbox, pname)\n",
    "\n",
    "    # second stage: create N_SIMULATIONS tasks to compute the n'th power\n",
    "    # of that number (this stage runs at least once)\n",
    "    s2 = get_stage_2(sandbox, pname)\n",
    "\n",
    "    # the third stage is added dynamically after convergence, so we return\n",
    "    # the pipeline with the initial two stages\n",
    "    p.add_stages(s1)\n",
    "    p.add_stages(s2)\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93008de1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T12:18:01.131849Z",
     "start_time": "2022-11-22T12:18:01.127959Z"
    }
   },
   "outputs": [],
   "source": [
    "%env RADICAL_LOG_LVL=OFF\n",
    "%env RADICAL_REPORT_ANIME=FALSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac289f3f",
   "metadata": {},
   "source": [
    "Now we write the ensemble application. We create an EnTK's application manager which executes our ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6099dc3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T12:18:05.184338Z",
     "start_time": "2022-11-22T12:18:05.116044Z"
    }
   },
   "outputs": [],
   "source": [
    "appman = re.AppManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d09986",
   "metadata": {},
   "source": [
    "We assign resource request description to the application manager using three mandatory keys: target resource, walltime, and number of cpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf0026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T12:18:12.436762Z",
     "start_time": "2022-11-22T12:18:12.430867Z"
    }
   },
   "outputs": [],
   "source": [
    "appman.resource_desc = {\n",
    "    'resource': 'local.localhost_test',\n",
    "    'walltime': 10,\n",
    "    'cpus'    : 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc2639",
   "metadata": {},
   "source": [
    "We create an ensemble of **n** simulation pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc5862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T12:18:16.824048Z",
     "start_time": "2022-11-22T12:18:16.782178Z"
    }
   },
   "outputs": [],
   "source": [
    "# create an ensemble of n simulation pipelines\n",
    "for cnt in range(N_PIPELINES):\n",
    "    pname = 'pipe.%03d' % cnt\n",
    "    generate_pipeline(pname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7733826a",
   "metadata": {},
   "source": [
    "We assign the workflow to the application manager, then run the ensemble and wait for completion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e42ba7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T12:20:19.608869Z",
     "start_time": "2022-11-22T12:18:19.196091Z"
    }
   },
   "outputs": [],
   "source": [
    "appman.workflow = set(ensemble.values())\n",
    "appman.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8376e48",
   "metadata": {},
   "source": [
    "We check results which were staged back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7694b4fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T12:20:36.183392Z",
     "start_time": "2022-11-22T12:20:36.176650Z"
    }
   },
   "outputs": [],
   "source": [
    "for cnt in range(N_PIPELINES):\n",
    "    result = int(open('pipe.%03d.sum.txt' % cnt).read())\n",
    "    print('%18d - %10d' % (cnt, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405c0a6-06fe-46e1-92c1-dbaafc5c0397",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "#### When doing the following exercises, please reset the kernel and clean all output before rerunning the notebook!\n",
    "\n",
    "Calculating intermediate results is costly: all data need to be staged back and analyzed. Instead, insert a `stage 2b` which analyzes data on the target resource and only then stages back the result to decide about pipeline continuation.\n",
    "\n",
    "  - This exercise is very similar to previous exercise 2\n",
    "  - `stage 2b` would be very similar to `stage 3`\n",
    "  - Consider what stage then needs to hold the `post_exec`\n",
    "\n",
    "*Note*: you can turn off the reporter output with \n",
    "`export RADICAL_REPORT=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b56fe8-375a-43bd-901a-1d98c6578975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
